#!/usr/bin/env python3
"""
Universal Chat CLI â€“ Python Edition
Pure stdlib, Python 3.8+. Zero pip installs required.

Usage:
    python ai.py <provider> [filter]...
    ./ai.py gemini
    ./ai.py openrouter 32b
    ./ai.py groq llama

Providers: gemini, openrouter, groq, together, cerebras, novita, ollama

Chat commands:
    /history            Show conversation history
    /save <name>        Save session to ~/.chat_sessions/<name>.json
    /load <name>        Load a saved session
    /clear              Delete all saved sessions
    /upload <path>      Attach an image to your next message
    /image              Show currently attached image
    /clearimage         Remove the attached image
    /togglethinking     Toggle reasoning/thinking output display
    /help               Show available commands
    quit / exit         End the session
"""

import sys
import os
import json
import re
import base64
import signal
import urllib.request
import urllib.error
import mimetypes
import shutil
import atexit
from pathlib import Path
from typing import Optional

# Try readline (not available on Windows without pyreadline3)
try:
    import readline
    _READLINE_AVAILABLE = True
except ImportError:
    _READLINE_AVAILABLE = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CONFIGURATION  â€“  Edit here
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

##############################################################################
#                   !!! EDIT YOUR API KEYS HERE !!!                          #
##############################################################################
API_KEYS: dict[str, str] = {
    "gemini":     "",   # https://aistudio.google.com/app/apikey
    "openrouter": "",   # https://openrouter.ai/keys
    "groq":       "",   # https://console.groq.com/keys
    "together":   "",   # https://api.together.ai/settings/api-keys
    "cerebras":   "",   # https://cloud.cerebras.ai/
    "novita":     "",   # https://novita.ai/
    "ollama":     "",   # https://ollama.com/ (leave blank for local)
}

# Conversation defaults
MAX_HISTORY_MESSAGES  = 20        # Max user+AI turns kept in context
MAX_MESSAGE_LENGTH    = 50_000    # Max chars for a single message
DEFAULT_TEMPERATURE   = 0.7       # 0â€“2: higher = more creative
DEFAULT_MAX_TOKENS    = 3000      # Max tokens in each AI reply
DEFAULT_TOP_P         = 0.9       # 0â€“1: nucleus sampling
SESSION_DIR           = Path.home() / ".chat_sessions"
HISTORY_FILE          = Path.home() / ".ai_cli_history"  # readline history

# Image support
MAX_IMAGE_SIZE_MB = 20
SUPPORTED_MIME_TYPES = {"image/jpeg", "image/png", "image/webp", "image/gif"}

# System prompt (set to "" to disable)
SYSTEM_PROMPT = "You are a helpful assistant running in a command-line interface."

# Toggle thinking/reasoning output on startup
ENABLE_THINKING_OUTPUT = True

# Request timeouts in seconds
REQUEST_TIMEOUT     = 300   # Streaming chat timeout
MODEL_FETCH_TIMEOUT = 30    # Model listing timeout

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ANSI COLOURS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class C:
    RESET  = "\033[0m"
    USER   = "\033[38;5;199m"   # bright magenta
    AI     = "\033[38;5;40m"    # bright green
    THINK  = "\033[38;5;214m"   # soft orange
    ERROR  = "\033[38;5;203m"   # vivid red
    WARN   = "\033[38;5;221m"   # soft yellow
    INFO   = "\033[38;5;75m"    # cyan-blue
    BOLD   = "\033[1m"
    IMAGE  = "\033[38;5;208m"   # orange
    CLR    = "\033[2K\r"        # clear current terminal line


def cprint(msg: str, end: str = "\n") -> None:
    """Print to stdout, flushing immediately."""
    sys.stdout.write(msg + end)
    sys.stdout.flush()


def eprint(msg: str) -> None:
    """Print to stderr (status / warning messages)."""
    sys.stderr.write(msg + "\n")
    sys.stderr.flush()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  PROVIDER ENDPOINTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ENDPOINTS: dict[str, dict[str, str]] = {
    "gemini": {
        "chat_base": "https://generativelanguage.googleapis.com/v1beta/models/",
        "models":    "https://generativelanguage.googleapis.com/v1beta/models",
    },
    "openrouter": {
        "chat":   "https://openrouter.ai/api/v1/chat/completions",
        "models": "https://openrouter.ai/api/v1/models",
    },
    "groq": {
        "chat":   "https://api.groq.com/openai/v1/chat/completions",
        "models": "https://api.groq.com/openai/v1/models",
    },
    "together": {
        "chat":   "https://api.together.ai/v1/chat/completions",
        "models": "https://api.together.ai/v1/models",
    },
    "cerebras": {
        "chat":   "https://api.cerebras.ai/v1/chat/completions",
        "models": "https://api.cerebras.ai/v1/models",
    },
    "novita": {
        "chat":   "https://api.novita.ai/v3/openai/chat/completions",
        "models": "https://api.novita.ai/v3/openai/models",
    },
    "ollama": {
        # Swap to http://localhost:11434 for local Ollama
        "chat":   "https://ollama.com/api/chat",
        "models": "https://ollama.com/api/tags",
    },
}

VALID_PROVIDERS = list(ENDPOINTS.keys())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def truncate(s: str, n: int) -> str:
    if not s:
        return ""
    return s[:n - 3] + "..." if len(s) > n else s


def validate_session_name(name: str) -> bool:
    if not re.fullmatch(r"[a-zA-Z0-9_-]+", name):
        eprint(f"{C.ERROR}Error: Session name may only contain letters, numbers, dash, underscore.{C.RESET}")
        return False
    if len(name) > 100:
        eprint(f"{C.ERROR}Error: Session name too long (max 100 chars).{C.RESET}")
        return False
    return True


def check_placeholder_key(key: str, provider: str) -> bool:
    """Returns True if key looks valid, False (with warning) otherwise."""
    msg = ""
    if not key:
        msg = "is empty"
    elif key.startswith("YOUR_") or key.endswith("-HERE") or "..." in key:
        msg = "appears to be a placeholder"
    elif provider == "gemini" and key == "-":
        msg = "is the default placeholder '-'"
    elif provider == "openrouter" and key == "sk-or-v1-":
        msg = "is an incomplete OpenRouter key"
    elif provider == "groq" and key.startswith("gsk_") and len(key) < 10:
        msg = "looks like an incomplete Groq key"
    elif provider == "cerebras" and key == "csk-":
        msg = "is the bare Cerebras prefix"
    elif provider in ("novita", "ollama") and len(key) < 10:
        msg = "is too short to be valid"

    if msg:
        eprint(f"{C.WARN}{'!' * 68}{C.RESET}")
        eprint(f"{C.WARN}!! WARNING: API key for '{provider.upper()}' {msg}.{C.RESET}")
        eprint(f"{C.WARN}!! Edit this script and set your real key in API_KEYS.{C.RESET}")
        eprint(f"{C.WARN}{'!' * 68}{C.RESET}")
        return False
    return True


def strip_think_tags(text: str) -> str:
    """Remove <think>...</think> blocks for clean history storage."""
    result, remaining = "", text
    while remaining:
        start = remaining.find("<think")
        if start == -1:
            result += remaining
            break
        result += remaining[:start]
        after_open = remaining[start + 6:]          # skip "<think"
        bracket = after_open.find(">")
        remaining = after_open[bracket + 1:] if bracket != -1 else ""
        close = remaining.find("</think")
        if close == -1:
            break                                    # unclosed â€“ drop rest
        after_close = remaining[close + 7:]
        bracket2 = after_close.find(">")
        remaining = after_close[bracket2 + 1:] if bracket2 != -1 else ""
    return result


def filter_models(models: list[str], filters: list[str]) -> list[str]:
    """Word-boundary filter: '3' matches 'gpt-3' but not '13b'."""
    if not filters:
        return models
    eprint(f"{C.INFO}Filtering with: {' '.join(filters)}  (word-boundary){C.RESET}")
    result = []
    for model in models:
        ml = model.lower()
        if all(
            re.search(r"(?:^|[^a-z0-9])" + re.escape(f.lower()) + r"(?:[^a-z0-9]|$)", ml)
            for f in filters
        ):
            result.append(model)
    return result


def _read_chunk(resp, size: int = 8192) -> bytes:
    """Read from response using read1() for instant delivery, with fallback."""
    try:
        return resp.read1(size)
    except AttributeError:
        # Fallback for environments where read1 is not available
        return resp.read(1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  IMAGE HANDLING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImageAttachment:
    def __init__(self):
        self.path   = ""
        self.base64 = ""
        self.mime   = ""

    def clear(self):
        self.path = self.base64 = self.mime = ""

    @property
    def attached(self) -> bool:
        return bool(self.base64)

    def load(self, raw_path: str) -> bool:
        """Validate, encode and store an image. Returns True on success."""
        path = Path(raw_path.strip("'\""))

        if not path.is_file():
            eprint(f"{C.ERROR}Error: File not found: {path}{C.RESET}")
            return False

        size_mb = path.stat().st_size / (1024 * 1024)
        if size_mb > MAX_IMAGE_SIZE_MB:
            eprint(f"{C.ERROR}Error: Image too large ({size_mb:.1f} MB). Max {MAX_IMAGE_SIZE_MB} MB.{C.RESET}")
            return False

        mime, _ = mimetypes.guess_type(str(path))
        # Fallback map for common extensions mimetypes might miss
        if not mime:
            ext_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg",
                       ".png": "image/png", ".gif": "image/gif",
                       ".webp": "image/webp"}
            mime = ext_map.get(path.suffix.lower(), "")

        if mime not in SUPPORTED_MIME_TYPES:
            eprint(f"{C.ERROR}Error: Unsupported image type '{mime}'. "
                   f"Supported: {', '.join(sorted(SUPPORTED_MIME_TYPES))}{C.RESET}")
            return False

        eprint(f"{C.IMAGE}Encoding imageâ€¦{C.RESET}")
        try:
            raw = path.read_bytes()
            self.base64 = base64.b64encode(raw).decode("ascii")
            self.mime   = mime
            self.path   = str(path)
            size_kb = len(raw) // 1024
            eprint(f"{C.IMAGE}âœ“ Attached: {path.name} ({mime}, {size_kb} KB){C.RESET}")
            return True
        except OSError as exc:
            eprint(f"{C.ERROR}Error reading image: {exc}{C.RESET}")
            return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  SESSION MANAGEMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _session_path(name: str) -> Path:
    return SESSION_DIR / f"{name}.json"


def save_session(name: str, history: list[dict]) -> None:
    SESSION_DIR.mkdir(parents=True, exist_ok=True)
    path = _session_path(name)
    try:
        path.write_text(json.dumps(history, ensure_ascii=False, indent=2), encoding="utf-8")
        cprint(f"{C.INFO}Session saved â†’ {path}{C.RESET}")
    except OSError as exc:
        eprint(f"{C.ERROR}Error saving session: {exc}{C.RESET}")


def _validate_session_data(data: object) -> str:
    """Return '' if valid, or an error description."""
    if not isinstance(data, list):
        return "not a JSON array"
    for msg in data:
        if not isinstance(msg, dict):
            return "element is not an object"
        role = msg.get("role")
        if not role:
            return "missing 'role' field"
        if role == "system":
            if not isinstance(msg.get("content"), str):
                return "system message missing string content"
        elif role in ("user", "assistant", "model"):
            has_content = isinstance(msg.get("content"), (str, list))
            has_parts   = isinstance(msg.get("parts"), list)
            if not has_content and not has_parts:
                return f"message role='{role}' has no content or parts"
        else:
            return f"unknown role '{role}'"
    return ""


def load_session(name: str) -> Optional[list[dict]]:
    SESSION_DIR.mkdir(parents=True, exist_ok=True)
    path = _session_path(name)
    if not path.exists():
        eprint(f"{C.ERROR}Error: Session not found: {path}{C.RESET}")
        return None
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except (OSError, json.JSONDecodeError) as exc:
        eprint(f"{C.ERROR}Error reading session: {exc}{C.RESET}")
        return None
    err = _validate_session_data(data)
    if err:
        eprint(f"{C.ERROR}Session is corrupt ({err}). Cannot load.{C.RESET}")
        return None
    cprint(f"{C.INFO}Session loaded â† {path}  ({len(data)} messages){C.RESET}")
    return data


def clear_sessions() -> None:
    if not SESSION_DIR.exists():
        cprint(f"{C.INFO}No saved sessions to clear.{C.RESET}")
        return
    files = sorted(SESSION_DIR.glob("*.json"))
    if not files:
        cprint(f"{C.INFO}No saved sessions to clear.{C.RESET}")
        return
    cprint(f"{C.WARN}This will permanently delete all sessions in {SESSION_DIR}:{C.RESET}")
    for f in files:
        cprint(f"  {f.stem}")
    try:
        answer = input(f"{C.WARN}Continue? (y/N): {C.RESET}").strip().lower()
    except (EOFError, KeyboardInterrupt):
        cprint(f"\n{C.INFO}Cancelled.{C.RESET}")
        return
    if answer == "y":
        for f in files:
            f.unlink(missing_ok=True)
        cprint(f"{C.INFO}All sessions cleared.{C.RESET}")
    else:
        cprint(f"{C.INFO}Cancelled.{C.RESET}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  HISTORY MANAGEMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def init_history(is_openai_compat: bool) -> list[dict]:
    if SYSTEM_PROMPT and is_openai_compat:
        return [{"role": "system", "content": SYSTEM_PROMPT}]
    return []


def truncate_history(history: list[dict], is_openai_compat: bool) -> list[dict]:
    """Trim history to MAX_HISTORY_MESSAGES conversational turns."""
    system_offset = 1 if (is_openai_compat and history and history[0].get("role") == "system") else 0
    max_total = MAX_HISTORY_MESSAGES + system_offset

    if len(history) <= max_total:
        return history

    to_remove = len(history) - max_total
    # Gemini requires strict user/model alternation â†’ remove in pairs
    if not is_openai_compat and to_remove % 2 == 1:
        to_remove += 1

    if system_offset:
        return [history[0]] + history[1 + to_remove:]
    return history[to_remove:]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MODEL FETCHING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_request(url: str, api_key: str, provider: str,
                   data: Optional[bytes] = None) -> urllib.request.Request:
    headers = {"Content-Type": "application/json", "Accept": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    if provider == "openrouter":
        headers["HTTP-Referer"] = "urn:chatcli:python"
        headers["X-Title"]      = "PythonChatCLI"
    method = "POST" if data else "GET"
    return urllib.request.Request(url, data=data, headers=headers, method=method)


def fetch_models(provider: str, api_key: str) -> Optional[list[str]]:
    ep = ENDPOINTS[provider]
    if provider == "gemini":
        url = f"{ep['models']}?key={api_key}"
        req = urllib.request.Request(url, method="GET")
    else:
        req = _build_request(ep["models"], api_key, provider)

    try:
        with urllib.request.urlopen(req, timeout=MODEL_FETCH_TIMEOUT) as resp:
            body = resp.read().decode("utf-8", errors="replace")
    except urllib.error.HTTPError as exc:
        body = exc.read().decode("utf-8", errors="replace")
        eprint(f"{C.ERROR}HTTP {exc.code} fetching models: {truncate(body, 200)}{C.RESET}")
        return None
    except OSError as exc:
        eprint(f"{C.ERROR}Network error fetching models: {exc}{C.RESET}")
        return None

    try:
        data = json.loads(body)
    except json.JSONDecodeError:
        eprint(f"{C.ERROR}Invalid JSON from model endpoint.{C.RESET}")
        eprint(f"{C.INFO}Raw (first 300): {truncate(body, 300)}{C.RESET}")
        return None

    # Surface API-level errors
    api_err = (data.get("error") if isinstance(data, dict) else None)
    if api_err:
        msg = api_err.get("message", str(api_err)) if isinstance(api_err, dict) else str(api_err)
        eprint(f"{C.ERROR}API error: {msg}{C.RESET}")
        return None

    # Extract model IDs per provider
    try:
        if provider == "gemini":
            models = [
                m["name"].replace("models/", "")
                for m in data.get("models", [])
                if (any("generateContent" in method
                        for method in m.get("supportedGenerationMethods", []))
                    and not m["name"].startswith("models/embedding"))
            ]
        elif provider == "ollama":
            models = [m["name"] for m in data.get("models", [])]
        elif provider == "together":
            arr = data if isinstance(data, list) else data.get("data", [])
            models = sorted(m["id"] for m in arr)
        else:
            models = sorted(m["id"] for m in data.get("data", []))
    except (KeyError, TypeError) as exc:
        eprint(f"{C.ERROR}Could not parse model list: {exc}{C.RESET}")
        eprint(f"{C.INFO}Raw (first 300): {truncate(body, 300)}{C.RESET}")
        return None

    return [m for m in models if m]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  PAYLOAD BUILDING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_user_message(text: str, image: ImageAttachment,
                       provider: str, is_openai_compat: bool,
                       is_first: bool) -> dict:
    """Construct the user turn dict, injecting system prompt for Gemini on turn 1."""
    prompt = text

    if image.attached:
        if not is_openai_compat:          # Gemini multimodal
            if is_first and SYSTEM_PROMPT:
                prompt = f"{SYSTEM_PROMPT}\n\n{text}"
            return {
                "role": "user",
                "parts": [
                    {"text": prompt},
                    {"inlineData": {"mimeType": image.mime, "data": image.base64}},
                ],
            }
        elif provider == "ollama":        # Ollama native images array
            return {"role": "user", "content": prompt, "images": [image.base64]}
        else:                             # OpenAI multimodal
            return {
                "role": "user",
                "content": [
                    {"type": "text", "text": text},
                    {"type": "image_url",
                     "image_url": {"url": f"data:{image.mime};base64,{image.base64}"}},
                ],
            }
    else:
        if not is_openai_compat:          # Gemini text-only
            if is_first and SYSTEM_PROMPT:
                prompt = f"{SYSTEM_PROMPT}\n\nUser: {text}"
            return {"role": "user", "parts": [{"text": prompt}]}
        else:
            return {"role": "user", "content": prompt}


def build_payload(provider: str, model_id: str,
                  history: list[dict], is_openai_compat: bool,
                  enable_tools: bool) -> dict:
    if not is_openai_compat:              # Gemini
        # Gemini doesn't use a "system" role entry
        contents = [m for m in history if m.get("role") != "system"]
        payload: dict = {
            "contents": contents,
            "generationConfig": {
                "temperature":    DEFAULT_TEMPERATURE,
                "maxOutputTokens": DEFAULT_MAX_TOKENS,
                "topP":           DEFAULT_TOP_P,
            },
        }
        if enable_tools:
            payload["tools"] = [{"urlContext": {}}, {"googleSearch": {}}]
        return payload

    # OpenAI-compatible base
    payload = {
        "model":       model_id,
        "messages":    history,
        "temperature": DEFAULT_TEMPERATURE,
        "stream":      True,
    }
    if provider == "ollama":
        payload["options"] = {"num_predict": DEFAULT_MAX_TOKENS, "top_p": DEFAULT_TOP_P}
    elif provider != "together":          # Together is sensitive to extra params
        payload["max_tokens"] = DEFAULT_MAX_TOKENS
        payload["top_p"]      = DEFAULT_TOP_P
    return payload


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  STREAMING RESPONSE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def stream_response(provider: str, model_id: str,
                    history: list[dict], is_openai_compat: bool,
                    api_key: str, enable_tools: bool,
                    enable_thinking: bool) -> Optional[str]:
    """
    Send the current history to the API and stream the reply to the terminal.
    Returns the assistant reply text (think-tags stripped) on success, or None.
    Handles Ctrl+C gracefully â€” interrupts the stream, returns partial text.
    """
    payload = build_payload(provider, model_id, history, is_openai_compat, enable_tools)
    payload_bytes = json.dumps(payload, ensure_ascii=False).encode("utf-8")

    ep = ENDPOINTS[provider]
    if not is_openai_compat:  # Gemini SSE endpoint
        url = f"{ep['chat_base']}{model_id}:streamGenerateContent?key={api_key}&alt=sse"
        req = urllib.request.Request(url, data=payload_bytes,
                                     headers={"Content-Type": "application/json"}, method="POST")
    else:
        req = _build_request(ep["chat"], api_key, provider, data=payload_bytes)

    sys.stdout.write(f"{C.AI}AI:{C.RESET} {C.INFO}(ğŸ’¬ Waitingâ€¦){C.RESET}")
    sys.stdout.flush()

    full_text       = ""   # accumulates everything (including think tags)
    full_thinking   = ""   # native thinking field
    first_chunk     = True
    is_thinking     = False
    in_think_disp   = False
    finish_reason   = ""
    error_msg       = ""
    interrupted     = False

    try:
        with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as resp:
            buffer = ""
            while True:
                # â”€â”€ Read with instant delivery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                try:
                    raw = _read_chunk(resp)
                except KeyboardInterrupt:
                    interrupted = True
                    cprint(f"\n{C.WARN}(Stream interrupted by user){C.RESET}")
                    break
                if not raw:
                    break
                buffer += raw.decode("utf-8", errors="replace")

                # Split on newlines, keep incomplete tail in buffer
                while "\n" in buffer:
                    line, buffer = buffer.split("\n", 1)
                    line = line.rstrip("\r")

                    # â”€â”€ Determine json_chunk from line â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    json_chunk = ""
                    if line.startswith("data: "):
                        json_chunk = line[6:].strip()
                        if json_chunk == "[DONE]":
                            break
                    elif line.startswith("{"):
                        json_chunk = line

                    if not json_chunk:
                        continue

                    try:
                        obj = json.loads(json_chunk)
                    except json.JSONDecodeError:
                        continue

                    # â”€â”€ Error check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    chunk_err = None
                    if isinstance(obj.get("error"), dict):
                        chunk_err = obj["error"].get("message", str(obj["error"]))
                    elif isinstance(obj.get("error"), str):
                        chunk_err = obj["error"]
                    elif isinstance(obj.get("detail"), str):
                        chunk_err = obj["detail"]
                    if chunk_err:
                        error_msg = f"API error: {chunk_err}"
                        break

                    # â”€â”€ Extract text / thinking / finish_reason â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    text_tok    = ""
                    think_tok   = ""
                    cur_finish  = ""

                    if is_openai_compat:
                        if provider == "ollama":
                            text_tok   = obj.get("message", {}).get("content") or ""
                            think_tok  = obj.get("message", {}).get("thinking") or ""
                            if obj.get("done") is True:
                                cur_finish = "stop"
                        else:
                            delta      = (obj.get("choices") or [{}])[0].get("delta", {})
                            text_tok   = delta.get("content") or ""
                            think_tok  = delta.get("reasoning") or ""
                            cur_finish = (obj.get("choices") or [{}])[0].get("finish_reason") or ""
                    else:
                        # Gemini
                        parts = ((obj.get("candidates") or [{}])[0]
                                 .get("content", {})
                                 .get("parts", [{}]))
                        text_tok   = parts[0].get("text", "") if parts else ""
                        cur_finish = (obj.get("candidates") or [{}])[0].get("finishReason", "")

                        # Safety block?
                        block = (obj.get("promptFeedback") or {}).get("blockReason")
                        if block:
                            error_msg = f"Content blocked (reason: {block})"
                            break

                        # Safety ratings check
                        if not cur_finish or cur_finish == "null":
                            safety_ratings = (obj.get("candidates") or [{}])[0].get("safetyRatings", [])
                            blocked_rating = next((r for r in safety_ratings if r.get("blocked")), None)
                            if blocked_rating:
                                cur_finish = "SAFETY"

                        # Tool calls (informational only)
                        if enable_tools:
                            tool_parts = [p for p in parts if "functionCall" in p]
                            if tool_parts:
                                if first_chunk:
                                    sys.stdout.write(C.CLR)
                                    sys.stdout.write(f"{C.AI}AI:{C.RESET}  ")
                                    first_chunk = False
                                cprint(f"\n{C.WARN}Tool call requested ğŸŒ:{C.RESET}")
                                for tp in tool_parts:
                                    cprint(json.dumps(tp, indent=2))
                                cprint(f"{C.WARN}(This script does not automatically execute "
                                       f"tool calls or return tool output to the model.){C.RESET}")

                    if cur_finish and not finish_reason:
                        finish_reason = cur_finish

                    # â”€â”€ Print first-chunk prefix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if first_chunk and (text_tok or think_tok):
                        sys.stdout.write(C.CLR)
                        sys.stdout.write(f"{C.AI}AI:{C.RESET}  ")
                        first_chunk = False

                    # â”€â”€ Native thinking field (Ollama / OpenAI reasoning)
                    if think_tok:
                        full_thinking += think_tok
                        if enable_thinking:
                            if not in_think_disp:
                                sys.stdout.write(f"{C.THINK}[Thinking] ")
                                in_think_disp = True
                            sys.stdout.write(f"{C.THINK}{think_tok}{C.RESET}")
                            sys.stdout.flush()

                    # â”€â”€ Text token with <think> tag state machine â”€â”€â”€â”€â”€â”€â”€
                    if text_tok:
                        full_text += text_tok
                        remaining = text_tok
                        while remaining:
                            if is_thinking:
                                close = remaining.find("</think")
                                if close != -1:
                                    before  = remaining[:close]
                                    after   = remaining[close + 7:]
                                    bracket = after.find(">")
                                    after   = after[bracket + 1:] if bracket != -1 else ""
                                    if enable_thinking:
                                        sys.stdout.write(before)
                                    sys.stdout.write(f"{C.RESET}\n{C.AI}")
                                    sys.stdout.flush()
                                    is_thinking   = False
                                    in_think_disp = False
                                    remaining     = after
                                else:
                                    if enable_thinking:
                                        sys.stdout.write(remaining)
                                        sys.stdout.flush()
                                    remaining = ""
                            else:
                                open_idx = remaining.find("<think")
                                if open_idx != -1:
                                    before  = remaining[:open_idx]
                                    after   = remaining[open_idx + 6:]
                                    bracket = after.find(">")
                                    after   = after[bracket + 1:] if bracket != -1 else ""
                                    sys.stdout.write(f"{C.AI}{before}")
                                    if enable_thinking:
                                        sys.stdout.write(f"{C.THINK}<think")
                                        in_think_disp = True
                                    is_thinking = True
                                    remaining   = after
                                else:
                                    if in_think_disp:
                                        sys.stdout.write(C.RESET)
                                        in_think_disp = False
                                    sys.stdout.write(f"{C.AI}{remaining}")
                                    sys.stdout.flush()
                                    remaining = ""

                    # â”€â”€ Gemini: non-standard finish reasons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if not is_openai_compat and finish_reason in ("SAFETY", "RECITATION", "OTHER"):
                        if not full_text:
                            error_msg = f"Stream ended by API (reason: {finish_reason})"
                        break

                    if provider == "ollama" and finish_reason == "stop":
                        break

    except KeyboardInterrupt:
        interrupted = True
        if first_chunk:
            sys.stdout.write(C.CLR)
        cprint(f"\n{C.WARN}(Request interrupted by user){C.RESET}")
    except urllib.error.HTTPError as exc:
        err_body = exc.read().decode("utf-8", errors="replace")
        if first_chunk:
            sys.stdout.write(C.CLR)
        error_msg = f"HTTP {exc.code}: {truncate(err_body, 200)}"
    except urllib.error.URLError as exc:
        if first_chunk:
            sys.stdout.write(C.CLR)
        error_msg = f"Network error: {exc.reason}"
    except OSError as exc:
        if first_chunk:
            sys.stdout.write(C.CLR)
        error_msg = f"Connection error: {exc}"

    # â”€â”€ Post-stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if first_chunk and not error_msg and not interrupted:
        sys.stdout.write(C.CLR)
        cprint(f"{C.AI}AI:{C.RESET} {C.INFO}(empty response){C.RESET}")
    elif not first_chunk:
        sys.stdout.write(f"{C.RESET}\n")
        sys.stdout.flush()

    if error_msg:
        cprint(f"{C.ERROR}{error_msg}{C.RESET}")
        return None

    # On interrupt, return whatever we got so far (if any)
    if interrupted and full_text:
        eprint(f"{C.INFO}(Partial response saved to history){C.RESET}")

    # Truncate oversized responses
    if len(full_text) > MAX_MESSAGE_LENGTH:
        eprint(f"{C.WARN}Response truncated at {MAX_MESSAGE_LENGTH} chars.{C.RESET}")
        full_text = full_text[:MAX_MESSAGE_LENGTH]

    clean = strip_think_tags(full_text)
    return clean if clean else None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  HISTORY DISPLAY HELPER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_display_text(msg: dict) -> str:
    """Extract readable text from any message format for /history display."""
    raw = msg.get("content") or msg.get("parts", [{}])

    if isinstance(raw, str):
        return raw

    if isinstance(raw, list) and raw:
        if isinstance(raw[0], dict):
            # Try Gemini parts format
            text = raw[0].get("text", "")
            if text:
                return text

            # Try OpenAI multimodal format
            for part in raw:
                if isinstance(part, dict) and part.get("type") == "text":
                    text = part.get("text", "")
                    if text:
                        return text

            # Check if there's an image attachment
            has_image = any(
                isinstance(p, dict) and
                (p.get("type") == "image_url" or "inlineData" in p)
                for p in raw
            )
            return "[ğŸ“ image]" if has_image else "[content]"

        return str(raw)

    return str(raw) if raw else "[empty]"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  USAGE / HELP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def print_usage() -> None:
    me = Path(sys.argv[0]).name
    cprint(f"""
{C.INFO}Usage:{C.RESET}
  python {me} <provider> [filter]...

{C.INFO}Providers:{C.RESET}
  gemini  openrouter  groq  together  cerebras  novita  ollama

{C.INFO}Chat commands:{C.RESET}
  {C.BOLD}/history{C.RESET}            Show conversation history
  {C.BOLD}/save <name>{C.RESET}        Save session  (~/.chat_sessions/<name>.json)
  {C.BOLD}/load <name>{C.RESET}        Load a saved session
  {C.BOLD}/clear{C.RESET}              Delete all saved sessions
  {C.BOLD}/upload <path>{C.RESET}      Attach an image {C.IMAGE}(vision models){C.RESET}
  {C.BOLD}/image{C.RESET}              Show attached image info
  {C.BOLD}/clearimage{C.RESET}         Remove attached image
  {C.BOLD}/togglethinking{C.RESET}     Toggle reasoning display {C.THINK}(reasoning models){C.RESET}
  {C.BOLD}/help{C.RESET}               Show this help
  {C.BOLD}quit{C.RESET} / {C.BOLD}exit{C.RESET}          End session

{C.INFO}Examples:{C.RESET}
  {C.AI}python {me} gemini{C.RESET}
  {C.AI}python {me} openrouter claude{C.RESET}
  {C.AI}python {me} groq llama{C.RESET}
  {C.AI}python {me} ollama{C.RESET}

{C.IMAGE}Image support:{C.RESET}
  Supports JPEG, PNG, GIF, WebP. Max {MAX_IMAGE_SIZE_MB} MB per image.
  Usage: {C.BOLD}/upload ~/photo.jpg{C.RESET}, then type your question.

{C.THINK}Thinking output:{C.RESET}
  Displays reasoning/thinking content from supported models in orange.
  Toggle with {C.BOLD}/togglethinking{C.RESET} during chat.

{C.WARN}Set your API keys in the API_KEYS dict at the top of this script.{C.RESET}""")


def print_chat_help() -> None:
    """Shorter help displayed during an active chat session."""
    cprint(f"""{C.INFO}Commands:{C.RESET}
  {C.BOLD}/history{C.RESET}            Show conversation
  {C.BOLD}/save <name>{C.RESET}        Save session
  {C.BOLD}/load <name>{C.RESET}        Load session
  {C.BOLD}/clear{C.RESET}              Delete all sessions
  {C.BOLD}/upload <path>{C.RESET}      Attach image
  {C.BOLD}/image{C.RESET}              Show attached image
  {C.BOLD}/clearimage{C.RESET}         Remove image
  {C.BOLD}/togglethinking{C.RESET}     Toggle reasoning display
  {C.BOLD}/help{C.RESET}               Show this help
  {C.BOLD}quit{C.RESET} / {C.BOLD}exit{C.RESET}          End session""")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MAIN CHAT LOOP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chat_loop(provider: str, model_id: str,
              is_openai_compat: bool, api_key: str,
              enable_tools: bool) -> None:

    # â”€â”€ readline setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if _READLINE_AVAILABLE:
        readline.set_history_length(1000)
        if HISTORY_FILE.exists():
            try:
                readline.read_history_file(str(HISTORY_FILE))
            except OSError:
                pass
        atexit.register(lambda: readline.write_history_file(str(HISTORY_FILE)))

    # â”€â”€ state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    history     : list[dict]      = init_history(is_openai_compat)
    image       : ImageAttachment = ImageAttachment()
    first_msg   : bool            = True
    thinking_on : bool            = ENABLE_THINKING_OUTPUT

    # â”€â”€ startup banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    sep = "â”€" * 85
    cprint(f"\n{sep}")
    cprint(f"  {C.INFO}Provider:{C.RESET}  {provider.upper()}   "
           f"{C.INFO}Model:{C.RESET}  {model_id}")
    cprint(f"  {C.INFO}History:{C.RESET}   last {MAX_HISTORY_MESSAGES} turns  â”‚  "
           f"{C.INFO}Temp:{C.RESET} {DEFAULT_TEMPERATURE}  â”‚  "
           f"{C.INFO}Tokens:{C.RESET} {DEFAULT_MAX_TOKENS}  â”‚  "
           f"{C.INFO}TopP:{C.RESET} {DEFAULT_TOP_P}")
    cprint(f"  {C.INFO}Max message:{C.RESET}  {MAX_MESSAGE_LENGTH:,} characters")
    if SYSTEM_PROMPT:
        label = "prepended to first message" if not is_openai_compat else "active"
        cprint(f"  {C.INFO}System prompt:{C.RESET}  {label}")
    else:
        cprint(f"  {C.INFO}System prompt:{C.RESET}  inactive (empty)")
    if provider == "gemini":
        status = f"{C.BOLD}enabled{C.RESET}" if enable_tools else "disabled"
        cprint(f"  {C.INFO}Tool calling:{C.RESET}  {status}")
    think_status = f"{C.BOLD}{C.THINK}enabled{C.RESET}" if thinking_on else "disabled"
    cprint(f"  {C.INFO}Thinking output:{C.RESET}  {think_status}  (toggle: /togglethinking)")
    cprint(f"  Type {C.BOLD}quit{C.RESET} or {C.BOLD}exit{C.RESET} to end  â”‚  "
           f"{C.BOLD}/help{C.RESET} for all commands")
    cprint(sep + "\n")

    # â”€â”€ REPL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    while True:
        # Build input prompt
        img_tag = (f"[{C.IMAGE}ğŸ“ {Path(image.path).name}{C.RESET}] "
                   if image.attached else "")
        prompt = f"{img_tag}{C.BOLD}{C.USER}You:{C.RESET} "

        try:
            user_input = input(prompt).strip()
        except (EOFError, KeyboardInterrupt):
            cprint(f"\n{C.INFO}Ending session.{C.RESET}")
            break

        # â”€â”€ Exit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if user_input.lower() in ("quit", "exit"):
            break

        # â”€â”€ Slash commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if user_input.startswith("/"):
            parts  = user_input.split(None, 1)
            cmd    = parts[0].lower()
            args   = parts[1].strip() if len(parts) > 1 else ""

            if cmd == "/help":
                print_chat_help()
                continue

            elif cmd == "/upload":
                if not args:
                    eprint(f"{C.IMAGE}Usage: /upload <image_path>{C.RESET}")
                else:
                    image.load(args)
                continue

            elif cmd == "/image":
                if image.attached:
                    cprint(f"{C.IMAGE}Attached: {image.path}  ({image.mime}){C.RESET}")
                else:
                    cprint(f"{C.IMAGE}No image attached.{C.RESET}")
                continue

            elif cmd == "/clearimage":
                image.clear()
                cprint(f"{C.IMAGE}Image cleared.{C.RESET}")
                continue

            elif cmd == "/togglethinking":
                thinking_on = not thinking_on
                state = f"{C.BOLD}{C.THINK}enabled{C.RESET}" if thinking_on else f"{C.BOLD}disabled{C.RESET}"
                cprint(f"{C.INFO}Thinking output {state}.{C.RESET}")
                continue

            elif cmd == "/history":
                cprint(f"{C.INFO}â”€â”€ History ({len(history)} messages) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{C.RESET}")
                if not history:
                    cprint("  (empty)")
                for msg in history:
                    role = msg.get("role", "?")
                    text = _extract_display_text(msg)
                    colour = (C.USER if role == "user"
                              else C.AI if role in ("assistant", "model")
                              else C.WARN)
                    cprint(f"  {colour}[{role}]{C.RESET}  {truncate(text, 500)}")
                cprint(f"{C.INFO}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{C.RESET}")
                continue

            elif cmd == "/save":
                if not args:
                    eprint(f"{C.WARN}Usage: /save <name>{C.RESET}")
                elif validate_session_name(args):
                    save_session(args, history)
                continue

            elif cmd == "/load":
                if not args:
                    eprint(f"{C.WARN}Usage: /load <name>{C.RESET}")
                elif validate_session_name(args):
                    loaded = load_session(args)
                    if loaded is not None:
                        history   = loaded
                        first_msg = False
                continue

            elif cmd == "/clear":
                clear_sessions()
                continue

            else:
                eprint(f"{C.WARN}Unknown command '{cmd}'. Type /help for a list.{C.RESET}")
                continue

        # â”€â”€ Guard: empty input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not user_input and not image.attached:
            continue

        # Default prompt when only an image is attached
        if not user_input and image.attached:
            user_input = "Describe this image in detail."

        # Length guard
        if len(user_input) > MAX_MESSAGE_LENGTH:
            eprint(f"{C.ERROR}Message too long "
                   f"({len(user_input):,} chars, max {MAX_MESSAGE_LENGTH:,}).{C.RESET}")
            continue

        eprint(f"{C.INFO}[Sendingâ€¦]{C.RESET}")

        # â”€â”€ Build user message â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        user_msg = build_user_message(user_input, image,
                                      provider, is_openai_compat, first_msg)
        image.clear()
        first_msg = False

        history.append(user_msg)
        history = truncate_history(history, is_openai_compat)

        # â”€â”€ Call API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ai_text = stream_response(provider, model_id, history,
                                  is_openai_compat, api_key,
                                  enable_tools, thinking_on)

        if ai_text:
            if not is_openai_compat:
                history.append({"role": "model", "parts": [{"text": ai_text}]})
            else:
                history.append({"role": "assistant", "content": ai_text})
        else:
            # Rollback the user message on failure
            if history and history[-1].get("role") == "user":
                history.pop()
                eprint(f"{C.WARN}(User message rolled back due to error){C.RESET}")

        cprint("")   # blank line between turns


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ENTRY POINT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    # Validate config
    for name, val, lo, hi in [
        ("DEFAULT_TEMPERATURE", DEFAULT_TEMPERATURE, 0, 2),
        ("DEFAULT_TOP_P",       DEFAULT_TOP_P,       0, 1),
        ("DEFAULT_MAX_TOKENS",  DEFAULT_MAX_TOKENS,  1, 1_000_000),
    ]:
        if not (lo <= val <= hi):
            sys.exit(f"Config error: {name}={val} must be between {lo} and {hi}")

    # Argument parsing
    argv = sys.argv[1:]
    if not argv or argv[0] in ("-h", "--help"):
        print_usage()
        sys.exit(0)

    provider = argv[0].lower()
    filters  = argv[1:]

    if provider not in VALID_PROVIDERS:
        cprint(f"{C.ERROR}Unknown provider '{provider}'. "
               f"Choose from: {', '.join(VALID_PROVIDERS)}{C.RESET}")
        sys.exit(1)

    api_key = API_KEYS.get(provider, "")
    if not check_placeholder_key(api_key, provider):
        sys.exit(1)

    # Determine compatibility mode
    is_openai_compat = (provider != "gemini")

    # â”€â”€ Optional: Gemini tool-calling prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    enable_tools = False
    if provider == "gemini":
        while True:
            try:
                ans = input(f"{C.INFO}Enable Gemini tool calling "
                            f"(web search, URL context)? (y/n): {C.RESET}").strip().lower()
            except (EOFError, KeyboardInterrupt):
                sys.exit(0)
            if ans in ("y", "1", "yes"):
                enable_tools = True
                cprint(f"{C.INFO}Tool calling enabled.{C.RESET}")
                break
            elif ans in ("n", "0", "no"):
                cprint(f"{C.INFO}Tool calling disabled.{C.RESET}")
                break
            else:
                eprint(f"{C.WARN}Please enter y or n.{C.RESET}")

    # â”€â”€ Fetch models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cprint(f"{C.INFO}Fetching models for {provider.upper()}â€¦{C.RESET}")
    models = fetch_models(provider, api_key)
    if not models:
        cprint(f"{C.ERROR}No models returned by {provider.upper()}.{C.RESET}")
        sys.exit(1)

    if filters:
        models = filter_models(models, filters)

    if not models:
        cprint(f"{C.ERROR}No models matched filter: {' '.join(filters)}{C.RESET}")
        cprint(f"{C.INFO}Tip: filters use word-boundary matching "
               f"('3' matches 'gpt-3' but not '13b').{C.RESET}")
        sys.exit(1)

    # â”€â”€ Model selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if len(models) == 1:
        model_id = models[0]
        cprint(f"{C.INFO}Auto-selected:{C.RESET} {model_id}")
    else:
        cprint(f"{C.INFO}Available models for {provider.upper()}:{C.RESET}")
        for i, m in enumerate(models, 1):
            cprint(f"  {C.BOLD}{i:3}{C.RESET}. {m}")
        while True:
            try:
                choice = input(f"{C.INFO}Select model number: {C.RESET}").strip()
            except (EOFError, KeyboardInterrupt):
                sys.exit(0)
            if choice.isdigit() and 1 <= int(choice) <= len(models):
                model_id = models[int(choice) - 1]
                break
            eprint(f"{C.WARN}Enter a number between 1 and {len(models)}.{C.RESET}")

    cprint(f"{C.INFO}Using model:{C.RESET} {model_id}\n")

    # â”€â”€ Graceful Ctrl-C outside of streaming â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _original_sigint = signal.getsignal(signal.SIGINT)

    def _sigint_handler(sig, frame):
        cprint(f"\n{C.WARN}Interrupted.{C.RESET}")
        sys.exit(130)

    signal.signal(signal.SIGINT, _sigint_handler)

    # â”€â”€ Start chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    chat_loop(provider, model_id, is_openai_compat, api_key, enable_tools)
    cprint("ğŸ‘‹ Session ended.")


if __name__ == "__main__":
    main()
